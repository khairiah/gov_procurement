{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8ed506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\edmun\\anaconda3\\lib\\site-packages (2.0.39)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\edmun\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\edmun\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\edmun\\anaconda3\\lib\\site-packages (2.9.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sqlalchemy_utils in c:\\users\\edmun\\anaconda3\\lib\\site-packages (0.41.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.3 in c:\\users\\edmun\\anaconda3\\lib\\site-packages (from sqlalchemy_utils) (2.0.39)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\edmun\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.3->sqlalchemy_utils) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\edmun\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.3->sqlalchemy_utils) (4.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 0) install once (if needed)\n",
    "# Install SQLAlchemy (open-source SQL toolkit and Object-Relational Mapping (ORM) library for Python)\n",
    "%pip install sqlalchemy\n",
    "# Install PostgreSQL driver \n",
    "%pip install psycopg2\n",
    "# Install add-on package for SQLAlchemy\n",
    "%pip install sqlalchemy_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76413091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re  # for regex\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mtick\n",
    "import time\n",
    "import os\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy_utils import create_database\n",
    "from sqlalchemy import text\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime, Boolean, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "datasetid = \"d_3f960c10fed6145404ca7b821f263b87\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47cd944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10000 rows (offset=0)\n",
      "Fetched 10000 rows (offset=10000)\n",
      "Fetched 10000 rows (offset=20000)\n",
      "Fetched 10000 rows (offset=30000)\n",
      "Fetched 10000 rows (offset=40000)\n",
      "Fetched 10000 rows (offset=50000)\n",
      "Fetched 10000 rows (offset=60000)\n",
      "Fetched 10000 rows (offset=70000)\n",
      "Fetched 10000 rows (offset=80000)\n",
      "Fetched 10000 rows (offset=90000)\n",
      "Fetched 10000 rows (offset=100000)\n",
      "Fetched 10000 rows (offset=110000)\n",
      "Fetched 10000 rows (offset=120000)\n",
      "Fetched 10000 rows (offset=130000)\n",
      "Fetched 10000 rows (offset=140000)\n",
      "Fetched 10000 rows (offset=150000)\n",
      "Fetched 10000 rows (offset=160000)\n",
      "Fetched 10000 rows (offset=170000)\n",
      "Fetched 10000 rows (offset=180000)\n",
      "Fetched 10000 rows (offset=190000)\n",
      "Fetched 10000 rows (offset=200000)\n",
      "Fetched 10000 rows (offset=210000)\n",
      "Fetched 10000 rows (offset=220000)\n",
      "Fetched 10000 rows (offset=230000)\n",
      "Fetched 10000 rows (offset=240000)\n",
      "Fetched 10000 rows (offset=250000)\n",
      "Fetched 10000 rows (offset=260000)\n",
      "Fetched 10000 rows (offset=270000)\n",
      "Fetched 10000 rows (offset=280000)\n",
      "Fetched 10000 rows (offset=290000)\n",
      "Fetched 10000 rows (offset=300000)\n",
      "Fetched 10000 rows (offset=310000)\n",
      "Fetched 10000 rows (offset=320000)\n",
      "Fetched 10000 rows (offset=330000)\n",
      "Fetched 10000 rows (offset=340000)\n",
      "Fetched 10000 rows (offset=350000)\n",
      "Fetched 10000 rows (offset=360000)\n",
      "Fetched 10000 rows (offset=370000)\n",
      "Fetched 10000 rows (offset=380000)\n",
      "Fetched 10000 rows (offset=390000)\n",
      "Fetched 10000 rows (offset=400000)\n",
      "Fetched 10000 rows (offset=410000)\n",
      "Fetched 10000 rows (offset=420000)\n",
      "Fetched 10000 rows (offset=430000)\n",
      "Fetched 10000 rows (offset=440000)\n",
      "Fetched 10000 rows (offset=450000)\n",
      "Fetched 10000 rows (offset=460000)\n",
      "Fetched 10000 rows (offset=470000)\n",
      "Fetched 10000 rows (offset=480000)\n",
      "Fetched 10000 rows (offset=490000)\n",
      "Fetched 10000 rows (offset=500000)\n",
      "Fetched 10000 rows (offset=510000)\n",
      "Fetched 10000 rows (offset=520000)\n",
      "Fetched 10000 rows (offset=530000)\n",
      "Fetched 10000 rows (offset=540000)\n",
      "Fetched 10000 rows (offset=550000)\n",
      "Fetched 10000 rows (offset=560000)\n",
      "Fetched 10000 rows (offset=570000)\n",
      "Fetched 10000 rows (offset=580000)\n",
      "Fetched 10000 rows (offset=590000)\n",
      "Fetched 10000 rows (offset=600000)\n",
      "Fetched 10000 rows (offset=610000)\n",
      "Fetched 10000 rows (offset=620000)\n",
      "Fetched 10000 rows (offset=630000)\n",
      "Fetched 10000 rows (offset=640000)\n",
      "Fetched 10000 rows (offset=650000)\n",
      "Fetched 10000 rows (offset=660000)\n",
      "Fetched 10000 rows (offset=670000)\n",
      "Fetched 10000 rows (offset=680000)\n",
      "Fetched 10000 rows (offset=690000)\n",
      "Fetched 10000 rows (offset=700000)\n",
      "Fetched 10000 rows (offset=710000)\n",
      "Fetched 10000 rows (offset=720000)\n",
      "Fetched 10000 rows (offset=730000)\n",
      "Fetched 10000 rows (offset=740000)\n",
      "Fetched 10000 rows (offset=750000)\n",
      "Fetched 10000 rows (offset=760000)\n",
      "Fetched 10000 rows (offset=770000)\n",
      "Fetched 10000 rows (offset=780000)\n",
      "Fetched 10000 rows (offset=790000)\n",
      "Fetched 10000 rows (offset=800000)\n",
      "Fetched 10000 rows (offset=810000)\n",
      "Fetched 10000 rows (offset=820000)\n",
      "Fetched 10000 rows (offset=830000)\n",
      "Fetched 10000 rows (offset=840000)\n",
      "Fetched 10000 rows (offset=850000)\n",
      "Fetched 10000 rows (offset=860000)\n",
      "Fetched 10000 rows (offset=870000)\n",
      "Fetched 10000 rows (offset=880000)\n",
      "Fetched 10000 rows (offset=890000)\n",
      "Fetched 10000 rows (offset=900000)\n",
      "Fetched 10000 rows (offset=910000)\n",
      "Fetched 10000 rows (offset=920000)\n",
      "Fetched 10000 rows (offset=930000)\n",
      "Fetched 10000 rows (offset=940000)\n",
      "Fetched 10000 rows (offset=950000)\n",
      "Fetched 10000 rows (offset=960000)\n",
      "Fetched 10000 rows (offset=970000)\n",
      "Fetched 10000 rows (offset=980000)\n",
      "Fetched 10000 rows (offset=990000)\n",
      "Fetched 10000 rows (offset=1000000)\n",
      "Fetched 10000 rows (offset=1010000)\n",
      "Fetched 10000 rows (offset=1020000)\n",
      "Fetched 10000 rows (offset=1030000)\n",
      "Fetched 10000 rows (offset=1040000)\n",
      "Fetched 10000 rows (offset=1050000)\n",
      "Fetched 10000 rows (offset=1060000)\n",
      "Fetched 10000 rows (offset=1070000)\n",
      "Fetched 10000 rows (offset=1080000)\n",
      "Fetched 10000 rows (offset=1090000)\n",
      "Fetched 10000 rows (offset=1100000)\n",
      "Fetched 10000 rows (offset=1110000)\n",
      "Fetched 10000 rows (offset=1120000)\n",
      "Fetched 10000 rows (offset=1130000)\n",
      "Fetched 10000 rows (offset=1140000)\n",
      "Fetched 10000 rows (offset=1150000)\n",
      "Fetched 10000 rows (offset=1160000)\n",
      "Fetched 10000 rows (offset=1170000)\n",
      "Fetched 10000 rows (offset=1180000)\n",
      "Fetched 10000 rows (offset=1190000)\n",
      "Fetched 10000 rows (offset=1200000)\n",
      "Fetched 10000 rows (offset=1210000)\n",
      "Fetched 10000 rows (offset=1220000)\n",
      "Fetched 10000 rows (offset=1230000)\n",
      "Fetched 10000 rows (offset=1240000)\n",
      "Fetched 10000 rows (offset=1250000)\n",
      "Fetched 10000 rows (offset=1260000)\n",
      "Fetched 10000 rows (offset=1270000)\n",
      "Fetched 10000 rows (offset=1280000)\n",
      "Fetched 10000 rows (offset=1290000)\n",
      "Fetched 10000 rows (offset=1300000)\n",
      "Fetched 10000 rows (offset=1310000)\n",
      "Fetched 10000 rows (offset=1320000)\n",
      "Fetched 10000 rows (offset=1330000)\n",
      "Fetched 10000 rows (offset=1340000)\n",
      "Fetched 10000 rows (offset=1350000)\n",
      "Fetched 10000 rows (offset=1360000)\n",
      "Fetched 10000 rows (offset=1370000)\n",
      "Fetched 10000 rows (offset=1380000)\n",
      "Fetched 10000 rows (offset=1390000)\n",
      "Fetched 10000 rows (offset=1400000)\n",
      "Fetched 10000 rows (offset=1410000)\n",
      "Fetched 10000 rows (offset=1420000)\n",
      "Fetched 10000 rows (offset=1430000)\n",
      "Fetched 10000 rows (offset=1440000)\n",
      "Fetched 10000 rows (offset=1450000)\n",
      "Fetched 10000 rows (offset=1460000)\n",
      "Fetched 10000 rows (offset=1470000)\n",
      "Fetched 10000 rows (offset=1480000)\n",
      "Fetched 10000 rows (offset=1490000)\n",
      "Fetched 10000 rows (offset=1500000)\n",
      "Fetched 10000 rows (offset=1510000)\n",
      "Fetched 10000 rows (offset=1520000)\n",
      "Fetched 10000 rows (offset=1530000)\n",
      "Fetched 10000 rows (offset=1540000)\n",
      "Fetched 10000 rows (offset=1550000)\n",
      "Fetched 10000 rows (offset=1560000)\n",
      "Fetched 10000 rows (offset=1570000)\n",
      "Fetched 10000 rows (offset=1580000)\n",
      "Fetched 10000 rows (offset=1590000)\n",
      "Fetched 10000 rows (offset=1600000)\n",
      "Fetched 10000 rows (offset=1610000)\n",
      "Fetched 10000 rows (offset=1620000)\n",
      "Fetched 10000 rows (offset=1630000)\n",
      "Fetched 10000 rows (offset=1640000)\n",
      "Fetched 10000 rows (offset=1650000)\n",
      "Fetched 10000 rows (offset=1660000)\n",
      "Fetched 4891 rows (offset=1670000)\n",
      "Total rows fetched: 1674891\n",
      "Columns: ['vault_id', 'uen', 'issuance_agency_desc', 'uen_status_desc', 'entity_name', 'entity_type_desc', 'uen_issue_date', 'reg_street_name', 'reg_postal_code']\n",
      "  vault_id         uen issuance_agency_desc uen_status_desc  \\\n",
      "0        1  201201936C                 ACRA    Deregistered   \n",
      "1        2   53250767C                 ACRA    Deregistered   \n",
      "2        3  199203392Z                 ACRA    Deregistered   \n",
      "3        4  201305833D                 ACRA    Deregistered   \n",
      "4        5  199600940G                 ACRA      Registered   \n",
      "\n",
      "                                 entity_name  \\\n",
      "0  MILLENNIUM AUTOMATION & SYSTEMS PTE. LTD.   \n",
      "1                    PARTY ART ENTERTAINMENT   \n",
      "2   DE COMFORT TRADING & DEVELOPMENT PTE LTD   \n",
      "3                    EXCEL CAPITAL PTE. LTD.   \n",
      "4              DYNASOURCE ENTERPRISE PTE LTD   \n",
      "\n",
      "                   entity_type_desc uen_issue_date         reg_street_name  \\\n",
      "0                     Local Company     2012-01-26        BENCOOLEN STREET   \n",
      "1  Sole Proprietorship/ Partnership     2013-12-13  CHOA CHU KANG AVENUE 4   \n",
      "2                     Local Company     1992-06-30              SEGAR ROAD   \n",
      "3                     Local Company     2013-03-06              ANSON ROAD   \n",
      "4                     Local Company     1996-02-06         PASIR RIS GROVE   \n",
      "\n",
      "  reg_postal_code  \n",
      "0          189648  \n",
      "1          680448  \n",
      "2          670481  \n",
      "3          079903  \n",
      "4          518217  \n"
     ]
    }
   ],
   "source": [
    "def fetch_dataset(datasetid, api_key, limit=10000):\n",
    "    url = f\"https://api-production.data.gov.sg/v2/public/api/datasets/{datasetid}/list-rows\"\n",
    "    headers = {\"x-api-key\": api_key}\n",
    "    \n",
    "    all_rows = []\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\"limit\": limit, \"offset\": offset}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        json_data = response.json()\n",
    "        rows = json_data[\"data\"][\"rows\"]\n",
    "        \n",
    "        if not rows:  # stop if no more data\n",
    "            break\n",
    "        \n",
    "        # Extract inner row dicts\n",
    "        records = [r.get(\"row\", r) for r in rows]\n",
    "        all_rows.extend(records)\n",
    "        \n",
    "        print(f\"Fetched {len(records)} rows (offset={offset})\")\n",
    "        \n",
    "        # Prepare for next batch\n",
    "        offset += limit\n",
    "    \n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "\n",
    "df = fetch_dataset(datasetid, api_key)\n",
    "print(\"Total rows fetched:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603e0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(os.getcwd(), \"acraEntities.csv\"), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc959eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vault_id',\n",
       " 'uen',\n",
       " 'issuance_agency_desc',\n",
       " 'uen_status_desc',\n",
       " 'entity_name',\n",
       " 'entity_type_desc',\n",
       " 'uen_issue_date',\n",
       " 'reg_street_name',\n",
       " 'reg_postal_code']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), \"acraEntities.csv\"))\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcf65f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vault_id                0\n",
       "uen                     0\n",
       "issuance_agency_desc    0\n",
       "uen_status_desc         0\n",
       "entity_name             1\n",
       "entity_type_desc        0\n",
       "uen_issue_date          0\n",
       "reg_street_name         0\n",
       "reg_postal_code         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "866ec980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674891\n",
      "1655655\n"
     ]
    }
   ],
   "source": [
    "print(df['uen'].nunique())\n",
    "print(df['entity_name'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e54692da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with double spaces: 4646\n",
      "Rows with double spaces: 4646\n",
      "Sample suppliers:\n",
      " 380783                            SANDSTONE CAPITAL PTE LTD\n",
      "782757                                  KALLANG0808 PTE LTD\n",
      "1523706                       MULTI FOOD INDUSTRIES PTE LTD\n",
      "1131541                                        KIA MUI & CO\n",
      "1482656                                   TEAM TREE PTE LTD\n",
      "515488                SEAHORSE INTERNATIONAL AGENCY PTE LTD\n",
      "448258                                MY LITTLE SWEET TOOTH\n",
      "842964                            PRYMO CONSULTANCY PTE LTD\n",
      "329417                                               STYLEQ\n",
      "1573058    COLOMBO RESTAURANT AUTHENTIC SHRI LANKAN CUISINE\n",
      "Name: entity_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1) Remove leading/trailing spaces make everything uppercase for consistency\n",
    "df['entity_name'] = df['entity_name'].str.strip()\n",
    "# 2) Collapse double/triple spaces \n",
    "\n",
    "# Identify rows with double spaces in entity_name column values\n",
    "mask = df['entity_name'].str.contains(r'\\s{2,}', na=False) #2 or more double spaces, na=False ignore NaN values if any\n",
    "df[mask].head(10)\n",
    "print(\"Rows with double spaces:\", mask.sum()) #if 0, means it's clean. otherwise, it means we have supplier names with double spaces\n",
    "\n",
    "# show distinct supplier names that have double spaces \n",
    "df.loc[mask, 'entity_name'].unique() \n",
    "\n",
    "# ensure supplier names are normalized. so no duplicates due to space issues\n",
    "df['entity_name'] = df['entity_name'].str.replace(r'\\s+', ' ', regex=True) \n",
    "print(\"Rows with double spaces:\", mask.sum()) #if 0, means it's clean. \n",
    "\n",
    "# 3) Standardize PTE LTD variants. \n",
    "# Note: Not ideal to strip PTE LTD as we would risk collisions. E.g. ABC PTE LTD (company) vs ABC LLP (partnership) run by diff owners\n",
    "\n",
    "# Normalizing common suffixes using regex (pattern matching)\n",
    "df['entity_name'] = df['entity_name'].str.replace(r'PTE\\.?', 'PTE', regex=True)\n",
    "df['entity_name'] = df['entity_name'].str.replace(r'LTD\\.?', 'LTD', regex=True)\n",
    "\n",
    "# Handling rare 'PTE LIMITED' \n",
    "df['entity_name'] = df['entity_name'].str.replace(r'PTE LIMITED', 'PTE LTD', regex=True)\n",
    "\n",
    "# Remove trailing periods. Some suppliers end with a dot\n",
    "df['entity_name'] = df['entity_name'].str.replace(r'\\.\\s*$', '', regex=True)\n",
    "\n",
    "# Add upper  case for consistency in SQL group buys\n",
    "df['entity_name'] = df['entity_name'].str.upper()\n",
    "\n",
    "# If award_amt = 0, entity_name = \"Unknown\", set entity_name to NULL in PostgreSQL\n",
    "\n",
    "print(\"Sample suppliers:\\n\", df['entity_name'].drop_duplicates().sample(10)) #to check if cleaning works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9d5783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned shape: (1674891, 9)\n",
      "Suspect duplicates shape: (0, 0)\n",
      "Columns still containing NaN/NaT: ['entity_name']\n",
      "entity_name    1\n",
      "dtype: int64\n",
      "         vault_id        uen issuance_agency_desc uen_status_desc entity_name  \\\n",
      "1277918   1277919  53347159A                 ACRA    Deregistered         NaN   \n",
      "\n",
      "                         entity_type_desc uen_issue_date  \\\n",
      "1277918  Sole Proprietorship/ Partnership     2016-09-30   \n",
      "\n",
      "              reg_street_name reg_postal_code  \n",
      "1277918  BEDOK SOUTH AVENUE 1          460003  \n"
     ]
    }
   ],
   "source": [
    "def clean_acra_data(df: pd.DataFrame):\n",
    "    # --- Drop internal columns ---\n",
    "    df = df.drop(columns=['_id', '__dataset_id'], errors='ignore')\n",
    "\n",
    "    # --- Normalize blanks to NaN ---\n",
    "    df = df.replace(r'^\\s*$', pd.NA, regex=True)\n",
    "\n",
    "    # --- Convert dates to datetime with explicit format ---\n",
    "    date_cols = [\n",
    "        'uen_issue_date'\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], format=\"%Y-%m-%d\", errors='coerce')\n",
    "\n",
    "    # --- Normalize UEN ---\n",
    "    if 'uen' in df.columns:\n",
    "        df['uen'] = df['uen'].str.strip().str.upper()\n",
    "\n",
    "    # --- Clean entity_name ---\n",
    "    if 'entity_name' in df.columns:\n",
    "        def normalize_entity_name(name: str) -> str:\n",
    "            if pd.isna(name):\n",
    "                return name\n",
    "            name = name.strip()\n",
    "\n",
    "            # collapse multiple spaces\n",
    "            name = re.sub(r'\\s+', ' ', name)\n",
    "\n",
    "            # upper case for standardization\n",
    "            name = name.upper()\n",
    "\n",
    "            # normalize common suffixes: remove dots in LTD, PTE. -> PTE\n",
    "            name = re.sub(r'\\bPTE\\.?\\b', 'PTE', name)\n",
    "            name = re.sub(r'\\bLTD\\.?\\b', 'LTD', name)\n",
    "\n",
    "            # remove trailing periods\n",
    "            name = re.sub(r'\\.+$', '', name)\n",
    "\n",
    "            return name\n",
    "\n",
    "        df['entity_name'] = df['entity_name'].apply(normalize_entity_name)\n",
    "\n",
    "    # --- Numeric cleaning ---\n",
    "    if 'no_of_officers' in df.columns:\n",
    "        df['no_of_officers'] = pd.to_numeric(df['no_of_officers'], errors='coerce')\n",
    "\n",
    "    # --- Identify suspicious duplicates ---\n",
    "    df_suspect_dupes = pd.DataFrame()\n",
    "    if 'uen' in df.columns and 'entity_name' in df.columns:\n",
    "        dupes = df[df.duplicated(subset=['uen'], keep=False)]\n",
    "        if not dupes.empty:\n",
    "            # group by UEN and keep only groups with more than 1 unique entity_name\n",
    "            suspect_groups = dupes.groupby('uen').filter(lambda g: g['entity_name'].nunique() > 1)\n",
    "            df_suspect_dupes = suspect_groups.sort_values('uen')\n",
    "\n",
    "    # --- Deduplicate based on (uen, entity_name, entity_status_description) ---\n",
    "    dedupe_cols = ['uen', 'entity_name', 'entity_status_description']\n",
    "    dedupe_cols = [c for c in dedupe_cols if c in df.columns]\n",
    "    if dedupe_cols:\n",
    "        df_cleaned = df.drop_duplicates(subset=dedupe_cols, keep='first')\n",
    "    else:\n",
    "        df_cleaned = df.copy()\n",
    "\n",
    "    return df_cleaned, df_suspect_dupes\n",
    "\n",
    "df_cleaned, df_suspect_dupes = clean_acra_data(df)\n",
    "\n",
    "print(\"Cleaned shape:\", df_cleaned.shape)\n",
    "print(\"Suspect duplicates shape:\", df_suspect_dupes.shape)\n",
    "\n",
    "# Example: review suspect duplicates\n",
    "df_suspect_dupes.head(20)\n",
    "\n",
    "# Check if NaN/NaT remain\n",
    "nan_columns = df_cleaned.columns[df_cleaned.isna().any()].tolist()\n",
    "print(\"Columns still containing NaN/NaT:\", nan_columns)\n",
    "print(df_cleaned[nan_columns].isna().sum())\n",
    "\n",
    "# Show a few example rows\n",
    "example_rows = df_cleaned[df_cleaned[nan_columns].isna().any(axis=1)].head(10)\n",
    "print(example_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4feb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, String, Date, Integer, create_engine\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Entity(Base):\n",
    "    __tablename__ = \"entities\"\n",
    "    \n",
    "    vault_id = Column(Integer, primary_key=True)\n",
    "    uen = Column(String, unique=True, nullable=False)\n",
    "    issuance_agency_desc = Column(String)\n",
    "    uen_status_desc = Column(String)\n",
    "    entity_name = Column(String)\n",
    "    entity_type_desc = Column(String)\n",
    "    uen_issue_date = Column(Date)\n",
    "    reg_street_name = Column(String)\n",
    "    reg_postal_code = Column(String)\n",
    "\n",
    "# connect\n",
    "engine = db.create_engine('postgresql+psycopg2://postgres:password@localhost:5432/procurement')\n",
    "\n",
    "# create table\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# insert from dataframe\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "records = df.to_dict(orient=\"records\")\n",
    "session.bulk_insert_mappings(Entity, records)\n",
    "session.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
